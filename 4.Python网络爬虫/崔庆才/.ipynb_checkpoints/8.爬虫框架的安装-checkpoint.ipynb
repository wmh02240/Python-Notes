{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.爬虫框架的安装\n",
    "    我们直接用 requests、Selenium 等库写爬虫，如果爬取量不是太大，速度要求不高，是完全可以满足需求的。但是写多了会发现其内部许多代码和组件是可以复用的，如果我们把这些组件抽离出来，将各个功能模块化，就慢慢会形成一个框架雏形，久而久之，爬虫框架就诞生了。利用框架，我们可以不用再去关心某些功能的具体实现，只需要关心爬取逻辑即可。有了它们，可以大大简化代码量，而且架构也会变得清晰，爬取效率也会高许多。所以，如果有一定的基础，上手框架是一种好的选择。主要介绍的爬虫框架有 pyspider 和 Scrapy及其扩展库的安装方式。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.pyspider 的安装\n",
    "    pyspider 是国人 binux 编写的强大的网络爬虫框架，它带有强大的 WebUI、脚本编辑器、任务监控器、项目管理器以及结果处理器，同时支持多种数据库后端、多种消息队列，另外还支持 JavaScript 渲染页面的爬取，使用起来非常方便，本节介绍一下它的安装过程。\n",
    "\n",
    "    官方文档：http://docs.pyspider.org/\n",
    "    PyPi：https://pypi.python.org/pypi/pyspider\n",
    "    GitHub：https://github.com/binux/pyspider\n",
    "    官方教程：http://docs.pyspider.org/en/latest/tutorial\n",
    "    在线实例：http://demo.pyspider.org\n",
    "    \n",
    "    pyspider 是支持 JavaScript 渲染的，而这个过程是依赖于 PhantomJS 的，所以还需要安装 PhantomJS（具体的安装过程详见 1.2.5 节）。\n",
    "    安装：pip3 install pyspider\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.Scrapy 的安装\n",
    "    Scrapy 是一个十分强大的爬虫框架，依赖的库比较多，至少需要依赖的库有 Twisted 14.0、lxml 3.4 和 pyOpenSSL 0.14。在不同的平台环境下，它所依赖的库也各不相同.\n",
    "    \n",
    "    官方网站：https://scrapy.org\n",
    "    官方文档：https://docs.scrapy.org\n",
    "    PyPi：https://pypi.python.org/pypi/Scrapy\n",
    "    GitHub：https://github.com/scrapy/scrapy\n",
    "    中文文档：http://scrapy-chs.readthedocs.io\n",
    "\n",
    "    安装：conda install Scrapy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.Scrapy-Splash 的安装\n",
    "    Scrapy-Splash 是一个 Scrapy 中支持 JavaScript 渲染的工具，Scrapy-Splash 的安装分为两部分。一个是 Splash 服务的安装，具体是通过 Docker，安装之后，会启动一个 Splash 服务，我们可以通过它的接口来实现 JavaScript 页面的加载。另外一个是 Scrapy-Splash 的 Python 库的安装，安装之后即可在 Scrapy 中使用 Splash 服务。\n",
    "\n",
    "    GitHub：https://github.com/scrapy-plugins/scrapy-splash\n",
    "    PyPi：https://pypi.python.org/pypi/scrapy-splash\n",
    "    使用说明：https://github.com/scrapy-plugins/scrapy-splash#configuration\n",
    "    Splash 官方文档：http://splash.readthedocs.io\n",
    "    \n",
    "    2. 安装 Splash\n",
    "    Scrapy-Splash 会使用 Splash 的 HTTP API 进行页面渲染，所以我们需要安装 Splash 来提供渲染服务。这里通过 Docker 安装，在这之前请确保已经正确安装好了 Docker。\n",
    "    安装命令如下：docker run -p 8050:8050 scrapinghub/splash\n",
    "    \n",
    "        2017-07-03 08:53:28+0000 [-] Log opened.  \n",
    "        2017-07-03 08:53:28.447291 [-] Splash version: 3.0  \n",
    "        2017-07-03 08:53:28.452698 [-] Qt 5.9.1, PyQt 5.9, WebKit 602.1, sip 4.19.3, Twisted 16.1.1, Lua 5.2  \n",
    "        2017-07-03 08:53:28.453120 [-] Python 3.5.2 (default, Nov 17 2016, 17:05:23) [GCC 5.4.0 20160609]  \n",
    "        2017-07-03 08:53:28.453676 [-] Open files limit: 1048576  \n",
    "        2017-07-03 08:53:28.454258 [-] Can't bump open files limit  \n",
    "        2017-07-03 08:53:28.571306 [-] Xvfb is started: ['Xvfb', ':1599197258', '-screen', '0', '1024x768x24',   \n",
    "            '-nolisten', 'tcp']  \n",
    "        QStandardPaths: XDG_RUNTIME_DIR not set, defaulting to '/tmp/runtime-root'  \n",
    "        2017-07-03 08:53:29.041973 [-] proxy profiles support is enabled, proxy profiles path:   \n",
    "            /etc/splash/proxy-profiles  \n",
    "        2017-07-03 08:53:29.315445 [-] verbosity=1  \n",
    "        2017-07-03 08:53:29.315629 [-] slots=50  \n",
    "        2017-07-03 08:53:29.315712 [-] argument_cache_max_entries=500  \n",
    "        2017-07-03 08:53:29.316564 [-] Web UI: enabled, Lua: enabled (sandbox: enabled)  \n",
    "        2017-07-03 08:53:29.317614 [-] Site starting on 8050  \n",
    "        2017-07-03 08:53:29.317801 [-] Starting factory &lt;twisted.web.server.Site object at 0x7ffaa4a98cf8&gt;\n",
    "\n",
    "    这样就证明 Splash 已经在 8050 端口上运行了。这时我们打开 http://localhost:8050, 即可看到Splash的主页。\n",
    "    \n",
    "#### 当然，Splash 也可以直接安装在远程服务器上。我们在服务器上以守护态运行 Splash 即可，命令如下：docker run -d -p 8050:8050 scrapinghub/splash\n",
    "    这里多了 -d 参数，它代表将 Docker 容器以守护态运行，这样在中断远程服务器连接后，不会终止 Splash 服务的运行。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.Scrapy-Splash 的安装\n",
    "    成功安装 Splash 之后，接下来再来安装其 Python 库，命令如下：pip install scrapy-splash\n",
    "    命令运行完毕后，就会成功安装好此库，后面会详细介绍它的用法。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.Scrapy-Redis 的安装\n",
    "\n",
    "    Scrapy-Redis是Scrapy的分布式扩展模块，有了它，我们就可以方便地实现Scrapy分布式爬虫的搭建。本节中，我们将介绍Scrapy-Redis的安装方式。\n",
    "    \n",
    "    GitHub：https://github.com/rmax/scrapy-redis\n",
    "    PyPi：https://pypi.python.org/pypi/scrapy-redis\n",
    "    官方文档：http://scrapy-redis.readthedocs.io\n",
    "    \n",
    "    安装：pip install scrapy-redis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy_redis"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.4 64-bit ('base': conda)",
   "language": "python",
   "name": "python37464bitbaseconda6107b79217b94e219b4f88edebad3871"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
